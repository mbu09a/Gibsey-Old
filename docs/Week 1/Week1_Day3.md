# Week 1 — Day 3 (Database Up & Seed One Shard)

> **Target session:** ≈ 2 h  **Focus:** bring Supabase/Postgres online, create `pages` table & enable `pgvector`; prove FastAPI can fetch one shard.
>
> **Outcome:** `/read?id=1` returns JSON from the database.

---

## 1 · Spin‑up Supabase project (cloud)

1. Log in to [https://app.supabase.com/](https://app.supabase.com/) → **New Project**.
2. **Name:** `gibsey-dev` **Password:** *generate & copy*.
3. Region: same coast as you; compute size: free tier is fine.
4. In the dashboard → **Project Settings → API** → copy:

   * `SUPABASE_URL`
   * `SUPABASE_ANON_KEY`

Append these to your local `.env` (not committed). Add placeholders to `.env.example`:

```env
SUPABASE_URL=https://xyz.supabase.co
SUPABASE_ANON_KEY=public‑anon‑key
```

Commit:

```bash
git add .env.example
git commit -m "chore: add Supabase keys placeholders"
```

---

## 2 · Create tables & enable pgvector

### Option A — Supabase SQL editor (fast)

Paste & run:

```sql
create extension if not exists vector;

create table if not exists pages (
  id        bigint generated by default as identity primary key,
  title     text not null,
  content   text not null,
  symbol_id int  not null default 1,
  embedding vector(1536) -- matches OpenAI v3-small
);
```

> **Note:** we’ll backfill `embedding` tomorrow; today it stays `NULL`.

### Option B — Local mirror (optional)

If you want a Dockerised Postgres that mirrors cloud SQL:

```bash
# already in compose; just expose vector extension
docker exec -it gibsey-dev-stack-db-1 psql -U gibsey -d gibsey -c "create extension if not exists vector;"
```

*(Sync scripts later; tonight cloud‑only is fine.)*

---

## 3 · Insert one shard manually

Grab shard text (≈ 100 words) for `id 1` — for now copy‑paste:

```sql
insert into pages (title, content, symbol_id)
values ('An Author\'s Preface — Shard 1', $$
I didn’t write this. I found it. This is a found text, and thus far, the identity of The Author—the original author—has yet to be definitively located.
$$, 1);
```

Verify:

```sql
select id, left(content, 60) || '…' as preview from pages limit 1;
```

---

## 4 · Add database client in backend

`apps/backend/app/db.py`:

```python
from supabase import create_client, Client
from .config import get_settings

class Supabase:
    _client: Client | None = None

    @classmethod
    def client(cls) -> Client:
        if cls._client is None:
            s = get_settings()
            cls._client = create_client(s.supabase_url, s.supabase_key)
        return cls._client
```

Update `app/config.py` settings:

```python
    supabase_url: str
    supabase_key: str
```

Add dependency in `requirements.txt`:

```
supabase>=2.0.0  # official Python client
python-dotenv
```

Re‑build backend image:

```bash
docker compose -f infra/compose.yaml build backend
```

---

## 5 · `/read` endpoint

In `app/main.py`:

```python
from fastapi import HTTPException, Query
from app.db import Supabase

@app.get("/read")
async def read_page(id: int = Query(..., gt=0)):
    page = (
        Supabase.client()
        .table("pages")
        .select("id,title,content,symbol_id")
        .eq("id", id)
        .single()
        .execute()
        .data
    )
    if not page:
        raise HTTPException(status_code=404, detail="Page not found")
    return page
```

Run stack:

```bash
docker compose -f infra/compose.yaml up -d
curl "http://localhost:8000/read?id=1" | jq .
```

Should return the shard JSON.

Add quick test `tests/test_read.py` (optional).

---

## 6 · CI update

In backend-lint job add installation of `supabase` so Ruff still passes; tests remain optional.

Commit & push as **day3-db-seed** branch:

```bash
git add apps/backend infra .github .env.example
git commit -m "feat: Supabase pages table + /read endpoint"
git push --set-upstream origin day3-db-seed
```

Open PR → link to *Day 3* issue → merge after green.

Move board card **“Day 3 – Supabase pages table”** ➜ *Done*.

---

### ✅ End‑of‑Day 3 Definition

* Supabase project exists with `pages` table & `vector` extension.
* One shard row present.
* `GET /read?id=1` delivers that shard from DB through FastAPI.

*Tomorrow (Day 4):* embed that shard & store vector column.
